# -*- coding: utf-8 -*-
"""Kaggle_Titanic_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gJrhHJ7GpcF33EN_Nx2IEnaq9JSIILYE
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

#  drive/My Drive/Kaggle/Data/titanic/train.csv

train_df = pd.read_csv('drive/My Drive/Kaggle/Data/titanic/train.csv')

train_df.head(20)

train_df.shape

train_df.info()

train_df.isna().sum()

train_df['Age'].median()     # we'll filll age missing values with their median

train_tmp = train_df.copy()

train_tmp.head()

# now let's create a function to preprocess the dataset

def preprocessing(dataframe):
    """
    take a pandas dataframe and preprocess it.
    do the following:-
    1. fill 'Age' column with thier median
    2. fill 'Embarked' column with value 'S'
    3. drop 'Ticket', 'Cabin', and 'Name' column
    4. convert 'Sex' and 'Embarked' col into numbers
    """
    dataset = dataframe.copy()

    # fill age colm with their median
    dataset['Age'].fillna(value=dataset['Age'].median(), inplace=True)

    # fill embarked column with S
    dataset['Embarked'].fillna(value='S', inplace=True)

    # let's drop Ticket and Cabin columns
    dataset.drop(['Ticket', 'Cabin', 'Name'], axis=1, inplace=True)

    # now we need to convert 'sex' and 'embarked' into numbers
    dataset['Sex'] = dataset['Sex'].astype('category')
    dataset['Embarked'] = dataset['Embarked'].astype('category')
    cat_columns = dataset.select_dtypes(['category']).columns
    # now change into numbers
    dataset[cat_columns] = dataset[cat_columns].apply(lambda x: x.cat.codes)

    return dataset

preprocessed_train_data = preprocessing(train_tmp)

preprocessed_train_data.info()

preprocessed_train_data.head()

"""**Do the same thing for test data**"""

test_df = pd.read_csv('drive/My Drive/Kaggle/Data/titanic/test.csv')

test_tmp = test_df.copy()

test_tmp.isna().sum()

# Let's fill one missing value in 'Fare' column with value=median
test_tmp['Fare'].fillna(value=test_tmp['Fare'].median(), inplace=True)

test_tmp.isna().sum()

preprocessed_test_data = preprocessing(test_tmp)

test_tmp.head()

preprocessed_test_data.head()

"""## Modelling

**Import all required library**
"""

# to split data
from sklearn.model_selection import train_test_split, cross_val_score

# models we're going to use
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression

# to evaluate and tuninig
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
from sklearn.metrics import confusion_matrix
from sklearn.metrics import precision_score, recall_score, f1_score

"""**split dataset into X and y**"""

X = preprocessed_train_data.drop('Survived', axis=1)
y = preprocessed_train_data['Survived']

X.shape, y.shape

"""**Split training data into training and validation set**"""

# set random seed
np.random.seed(42)

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)

X_train.shape, X_val.shape, y_train.shape, y_val.shape

# let's create a dictionary of all models and instantiate
models = {'Logistic Regression': LogisticRegression(),
          'KNN': KNeighborsClassifier(),
          'Random Forest': RandomForestClassifier()}

# let's create a function to fit the model and calculate the score

def fit_and_score(models, X_train, X_val, y_train, y_val):
    """
    take a dictionary of models and fit all the model on training data,
    and calculate the score on validation data
    and then return a dictionary of all model's scores
    """
    model_scores = {}
    for name, model in models.items():
        model.fit(X_train, y_train)  # fitting the model
        model_scores[name] = model.score(X_val, y_val)  # scoring
    return model_scores

model_scores = fit_and_score(models, X_train, X_val, y_train, y_val)

model_scores     # note: ignore all the WARNINGS

model_scores

# let's compare the model
pd.DataFrame(model_scores, index=['accuracy']).T.plot.bar();

"""for now onwards we'll consider only Logistic Regression and Random Forest

### Hyperparameter tuning

**RandomizedSearchCV**
"""

# Create a hyperparameter grid for LogisticRegression
log_reg_grid = {"C": np.logspace(-4, 4, 20),
                "solver": ["liblinear"]}

# Create a hyperparameter grid for RandomForestClassifier
rf_grid = {"n_estimators": np.arange(10, 1000, 50),
           "max_depth": [None, 3, 5, 10],
           "min_samples_split": np.arange(2, 20, 2),
           "min_samples_leaf": np.arange(1, 20, 2)}

# let's create randomized search models

rs_models = {'rs_log_reg': RandomizedSearchCV(estimator=LogisticRegression(),
                                             param_distributions=log_reg_grid,
                                             cv=5,
                                             n_iter=20,
                                             verbose=True),
            'rs_random_forest': RandomizedSearchCV(estimator=RandomForestClassifier(),
                                                   param_distributions=rf_grid,
                                                   cv=5,
                                                   n_iter=20,
                                                   verbose=True)
            }

rs_model_scores = fit_and_score(rs_models, X_train, X_val, y_train, y_val)

rs_model_scores

rs_models['rs_log_reg'].best_params_

rs_models['rs_random_forest'].best_params_

"""**GridSearchCV**

till now our `RandomForestClassifier` gives the best result we'll improve it using `GridSearchCV`
"""

gs_rf_grid = {"n_estimators": np.arange(10, 500, 50),
           "max_depth": [None, 10],
           "min_samples_split": np.arange(2, 6, 2),
           "min_samples_leaf": np.arange(2, 6, 2)}

gs_rf_model = GridSearchCV(estimator=RandomForestClassifier(),
                           param_grid=gs_rf_grid,
                           cv=5,
                           n_jobs=10,
                           verbose=True)
gs_rf_model.fit(X_train, y_train)

gs_rf_model.score(X_val, y_val)

gs_rf_model.best_params_

"""OKAY! let's create a new model with best_params and fit on complete training data and make predictions on test data"""

X_train = X
y_train = y
X_test = preprocessed_test_data

X_train.shape, y_train.shape, X_test.shape

new_model = RandomForestClassifier(n_estimators=110,
                                   max_depth=10,
                                   min_samples_leaf=2,
                                   min_samples_split=2)
new_model.fit(X_train, y_train)

predictions = new_model.predict(X_test)

predictions

"""# Create dataframe for submission"""

submission_type = pd.read_csv('drive/My Drive/Kaggle/Data/titanic/gender_submission.csv')

submission_type

submission = pd.DataFrame()

submission['PassengerId'] = X_test['PassengerId']
submission['Survived'] = predictions

submission

submission.to_csv('drive/My Drive/Kaggle/Data/titanic/titanic_submission.csv', index=False)

"""THANK YOU!!!"""

